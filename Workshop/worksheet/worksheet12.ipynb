{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90051 Workshop 12\n",
    "## Gaussian mixture models\n",
    "***\n",
    "In this worksheet we explore Gaussian mixture models (GMM). By the end of the worksheet, you should be able to:\n",
    "* generate data from a GMM;\n",
    "* fit GMMs using scikit-learn; and\n",
    "* select an appropriate value for the number of mixture components using a model selection criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 108\n",
    "np.random.seed(90051)\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sampling from a Gaussian mixture\n",
    "\n",
    "Consider a data set $\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_n\\}$ (each $\\mathbf{x}_i \\in \\mathbb{R}^m$) which we'd like to partition into clusters.\n",
    "\n",
    "A Gaussian mixture model (GMM) assumes that each instance $\\mathbf{x}_i$ is distributed according to a weighted mixture of multivariate normal distributions (Gaussians):\n",
    "$$\n",
    "\\mathbf{x}_i \\overset{\\mathrm{iid}}{\\sim} \\sum_{c = 1}^{k} w_c \\mathcal{N}(\\mathbf{\\mu}_c, \\mathbf{\\Sigma}_c)\n",
    "$$\n",
    "where $\\mathbf{\\mu}_c \\in \\mathbb{R}_m$ and $\\mathbf{\\Sigma}_c \\in \\mathbb{R}^{m \\times m}$ are the mean and covariance matrix for component $c$ and $\\sum_{c = 1}^{k} w_c = 1$.\n",
    "\n",
    "In this section, we'll write a function to generate a synthetic data set for a given set of GMM parameters.\n",
    "Later on, we'll try to fit a GMM to the synthetic data set and see how well we can recover the parameters.\n",
    "\n",
    "We'll store the GMM parameters in NumPy arrays as follows. Note: the zero-th axis indexes the component $c$ for all arrays.\n",
    "* `weights`: a 1D array $[w_1, \\ldots, w_k]$\n",
    "* `means`: a 2D array $[\\mathbf{\\mu}_1, \\ldots, \\mathbf{\\mu}_k]$\n",
    "* `covariances`: a 3D array $[\\mathbf{\\Sigma}_1, \\ldots, \\mathbf{\\Sigma}_k]$\n",
    "\n",
    "Below are some example parameters for a 2D feature space ($m = 2$) with $k = 3$ components. Note that the covariance matrices must be symmetric positive semi-definite. Thus each covariance matrix only has 3 degrees of freedom (for $m = 2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([0.5, 0.3, 0.2])\n",
    "\n",
    "means = np.array([[0, 0],    # mean of 1st component\n",
    "                  [50, 60],  # mean of 2nd component\n",
    "                  [0, 100]]) # mean of 3rd component\n",
    "\n",
    "covariances = np.array([[[160, 20], [20, 180]],  # covariance matrix of 1st component\n",
    "                        [[170, 30], [30, 120]],  # covariance matrix of 2nd component \n",
    "                        [[130, 40], [40, 130]]]) # covariance matrix of 3rd component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Exercise:** Complete the data generation function below.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gmm_data(n_instances, weights, means, covariances, debug=False):\n",
    "    \"\"\"\n",
    "    Generate data from a GMM\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_instances : int\n",
    "        number of instances in the generated data set\n",
    "    weights : numpy array, shape: (n_components,)\n",
    "        normalised component weights\n",
    "    means : numpy array, shape (n_components, n_features)\n",
    "        component means\n",
    "    covariances : numpy array, shape (n_components, n_features, n_features)\n",
    "        component covariance matrices\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy array, shape (n_instances, n_features)\n",
    "        data matrix\n",
    "    \"\"\"\n",
    "    n_components, n_features = means.shape\n",
    "    data = np.empty((0, n_features), dtype=np.double)\n",
    "    \n",
    "    # Draw number of instances in each component\n",
    "    counts = ... # fill in\n",
    "    \n",
    "    for c in range(0, n_components):\n",
    "        # Debugging info\n",
    "        if debug:\n",
    "            print('--- Component c={} ---'.format(c+1))\n",
    "            print('Mean: {}'.format(means[c]))\n",
    "            print('Covariance matrix:\\n {}'.format(covariances[c]))\n",
    "            print('Appending {} samples\\n'.format(counts[c]))\n",
    "        \n",
    "        # Draw x_i's for this component\n",
    "        cData = ... # fill in\n",
    "        \n",
    "        # Append to data\n",
    "        data = ... # fill in\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out (setting $n = 100$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_gmm_data(100, weights, means, covariances, debug=True)\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fitting a GMM\n",
    "\n",
    "Let's fit a Gaussian mixture model (GMM) to the synthetic data we've just generated.\n",
    "We'll use the built-in implementation in sklearn available at `sklearn.mixture.GaussianMixture`. \n",
    "The fitting procedure implements the Expectation Maximisation algorithm covered in lectures.\n",
    "\n",
    "We have to specify the number of clusters $k$â€”at first let's specify the true number. \n",
    "The `covariance_type` parameter allows one to make fitting more efficient by, e.g., restricting Gaussians to spherical shapes. \n",
    "In our case, we don't impose any additional restrictions on the covariance matrices, and hence use the 'full' option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=3, covariance_type='full')\n",
    "gmm.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the estimated parameters for the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('weights:\\n {}\\n'.format(gmm.weights_))\n",
    "print('means:\\n {}\\n'.format(gmm.means_))\n",
    "print('covariances:\\n {}\\n'.format(gmm.covariances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Question:** How do they compare with the \"true\" parameters?\n",
    "***\n",
    "\n",
    "For 2D data, we can also visualise the fitted model. \n",
    "The 2D Gaussians can be represented with isoline ellipsoids. \n",
    "For each Gaussian component, the ellipsoid is a location of points that have the same probability. \n",
    "\n",
    "Plotting an ellipsoid for a given 2D Gaussian, is somewhat non-trivial, and we are going to use a function developed for this purpose. \n",
    "Understanding the code and theory of function *plot_cov_ellipse* is not necessary for this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from http://www.nhsilbert.net/source/2014/06/bivariate-normal-ellipse-plotting-in-python/\n",
    "# and https://github.com/joferkington/oost_paper_code/blob/master/error_ellipse.py\n",
    "def plot_cov_ellipse(cov, pos, nstd=2, ax=None, fc='none', ec=[0,0,0], a=1, lw=2):\n",
    "    \"\"\"\n",
    "    Plots an `nstd` sigma error ellipse based on the specified covariance\n",
    "    matrix (`cov`). Additional keyword arguments are passed on to the \n",
    "    ellipse patch artist.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        cov : The 2x2 covariance matrix to base the ellipse on\n",
    "        pos : The location of the center of the ellipse. Expects a 2-element\n",
    "            sequence of [x0, y0].\n",
    "        nstd : The radius of the ellipse in numbers of standard deviations.\n",
    "            Defaults to 2 standard deviations.\n",
    "        ax : The axis that the ellipse will be plotted on. Defaults to the \n",
    "            current axis.\n",
    "        Additional keyword arguments are pass on to the ellipse patch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A matplotlib ellipse artist\n",
    "    \"\"\"\n",
    "    from scipy.stats import chi2\n",
    "    from matplotlib.patches import Ellipse\n",
    "    \n",
    "    def eigsorted(cov):\n",
    "        vals, vecs = np.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]\n",
    "        return vals[order], vecs[:,order]\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    vals, vecs = eigsorted(cov)\n",
    "    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "    \n",
    "    kwrg = {'facecolor':fc, 'edgecolor':ec, 'alpha':a, 'linewidth':lw}\n",
    "\n",
    "    # Width and height are \"full\" widths, not radius\n",
    "    width, height = 2 * nstd * np.sqrt(vals)\n",
    "    ellip = Ellipse(xy=pos, width=width, height=height, angle=theta, **kwrg)\n",
    "\n",
    "    ax.add_artist(ellip)\n",
    "    return ellip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Exercise:** Using the above function, implement visualisation that plots data overlaid with fitted Gaussian ellipsoids.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gmm(data, gmm):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy array, shape: (n_instances, n_features)\n",
    "        data matrix\n",
    "    \n",
    "    gmm : GaussianMixture\n",
    "        GaussianMixture instance to use for predictions/plotting\n",
    "    \"\"\"\n",
    "    # your code here #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gmm(data, gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualise how the 2D feature space is partitioned into clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 0.1   # grid resolution\n",
    "border = 5  # grid border\n",
    "\n",
    "x0_lower = np.min(data[:,0]) - border\n",
    "x0_upper = np.max(data[:,0]) + border\n",
    "x1_lower = np.min(data[:,1]) - border\n",
    "x1_upper = np.max(data[:,1]) + border\n",
    "\n",
    "x0, x1 = np.mgrid[x0_lower:x0_upper:res, x1_lower:x1_upper:res]\n",
    "grid = np.c_[x0.ravel(), x1.ravel()]\n",
    "cluster = gmm.predict(grid).reshape(x0.shape)\n",
    "\n",
    "plt.contourf(x0, x1, cluster)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we specify the \"wrong\" number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_2 = GaussianMixture(n_components=2, covariance_type='full').fit(data)\n",
    "plot_gmm(data, gmm_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_5 = GaussianMixture(n_components=5, covariance_type='full').fit(data)\n",
    "plot_gmm(data, gmm_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Exercise:** Repeat this section for different synthetic datasets, e.g. with overlapping clusters.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tuning the number of clusters\n",
    "\n",
    "In the previous section, we saw that it's important to select an appropriate value for $k$â€”i.e. GMM is not reslient to misspecified $k$.\n",
    "\n",
    "We'll now review the following methods for selecting $k$:\n",
    "1. **Log-likelihood based.** Requires held-out data. Can be computed for a data matrix `X` using `gmm.score(X)`.\n",
    "2. **AIC.** Akaike information criterion. Can be computed for a data matrix `X` using `gmm.aic(X)`.\n",
    "3. **BIC.** Bayesian information criterion. Can be computed for a data matrix `X` using `gmm.bic(X)`.\n",
    "\n",
    "But first, let's generate a more complicated data set with a larger number of clusters. Note that we partition the data into train/validation so that we can apply the first method above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([0.3, 0.2, 0.3, 0.1, 0.1])\n",
    "\n",
    "means = np.array([[0, 0], \n",
    "                  [50, 60], \n",
    "                  [0, 100], \n",
    "                  [100, -20], \n",
    "                  [-20, 40]])\n",
    "\n",
    "covariances = np.array([[[160, 20], [20, 180]], \n",
    "                        [[170, 30], [30, 120]], \n",
    "                        [[130, 40], [40, 130]], \n",
    "                        [[130, 40], [40, 130]], \n",
    "                        [[130, 40], [40, 130]]])\n",
    "\n",
    "data = generate_gmm_data(200, weights, means, covariances)\n",
    "\n",
    "train_data, validation_data = train_test_split(data, test_size=0.5)\n",
    "\n",
    "plt.scatter(train_data[:,0], train_data[:,1], marker='.', label='Train')\n",
    "plt.scatter(validation_data[:,0], validation_data[:,1], marker='*', label='Validation')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block below, you should fit a GMM for each value of $k \\in \\{1,\\ldots, 10\\}$ and compute:\n",
    "* `train_ll`: log-likelihood on the training set\n",
    "* `validation_ll`: log-likelihood on the validation set\n",
    "* `aic`: AIC on the training set\n",
    "* `aic_corrected`: corrected AIC (defined below) on the training set\n",
    "* `bic`: BIC on the training set\n",
    "* `n_parameters`: number of free parameters in the model\n",
    "\n",
    "Note that in practice it's recommended to use corrected AIC for small sample sizes. It's defined as \n",
    "$$\n",
    "\\mathrm{AICc} = \\mathrm{AIC} + \\frac{2 n_{\\mathrm{par}} (n_{\\mathrm{par}}+1)}{n-n_{\\mathrm{par}}-1}\n",
    "$$\n",
    "where $n_{\\mathrm{par}}$ is the number of free parameters, and $n$ is the number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_k = np.arange(1, 10, dtype=int)\n",
    "n_instances = data.shape[0]\n",
    "\n",
    "# Arrays to hold quantities for each k\n",
    "train_ll = np.zeros(range_k.size)\n",
    "validation_ll = np.zeros(range_k.size)\n",
    "aic = np.zeros(range_k.size)\n",
    "aic_corrected = np.zeros(range_k.size)\n",
    "bic = np.zeros(range_k.size)\n",
    "n_parameters = np.zeros(range_k.size)\n",
    "\n",
    "for i,k in enumerate(range_k):\n",
    "    # your code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot all these quantities as a function of $k$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range_k, train_ll, 'b.-', label = 'Train')\n",
    "plt.plot(range_k, validation_ll, 'k.-', label = 'Validation')\n",
    "plt.xlabel('number of components, $k$')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range_k, n_parameters, 'c.-', label = '# parameters')\n",
    "plt.plot(range_k, np.full_like(n_parameters, n_instances), 'y.-', label = '# instances')\n",
    "plt.xlabel('number of components, $k$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range_k, aic, 'r.-', label='AIC')\n",
    "plt.plot(range_k, aic_corrected, 'm.-', label='AICc')\n",
    "plt.plot(range_k, bic, 'g.-', label='BIC')\n",
    "plt.xlabel('number of components, $k$')\n",
    "plt.ylabel('score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Question:** Analyse the resulting plots. What can you tell about the number of parameters? Can all of these quantities be used to estimate the number of clusters?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extension: exploring the connection between GMMs and k-means clustering\n",
    "\n",
    "In lectures we saw that inference for a GMM can be performed using the expectation-maximisation (EM) algorithm. It is a coordinate ascent algorithm, which alternates between the following two steps:\n",
    "\n",
    "* **E-step:** Update the cluster responsibilities given the estimates of the model parameters at the $t$-th step $\\theta^{(t)} = (w_1^{(t)}, \\boldsymbol{\\mu}_1^{(t)}, \\boldsymbol{\\Sigma}_1^{(t)}, \\ldots, w_k^{(t)}, \\boldsymbol{\\mu}_k^{(t)}, \\boldsymbol{\\Sigma}_k^{(t)})$:\n",
    "$$\n",
    "r_{ic}^{(t + 1)} = p(z_i = c|\\mathbf{x}_i, \\theta^{(t)}) = \\frac{w_c^{(t)} \\mathcal{N}(\\mathbf{x}_i|\\boldsymbol{\\mu}_c^{(t)}, \\boldsymbol{\\Sigma}_c^{(t)})}{\\sum_{c'=1}^{k} w_{c'}^{(t)} \\mathcal{N}(\\mathbf{x}_i|\\boldsymbol{\\mu}_{c'}^{(t)}, \\boldsymbol{\\Sigma}_{c'}^{(t)})}\n",
    "$$\n",
    "\n",
    "* **M-step:** Update the model parameters using the estimates of the cluster responsibilities at step $t+1$:\n",
    "$$\n",
    "\\begin{align}\n",
    "w_c^{(t+1)} &= \\frac{\\sum_{i = 1}^{n} r_{ic}^{(t)}}{n} \\\\\n",
    "\\boldsymbol{\\mu}_c^{(t+1)} &= \\frac{\\sum_{i = 1}^{n} r_{ic}^{(t + 1)} \\mathbf{x}_i}{\\sum_{i = 1}^{n} r_{ic}^{(t)}} \\\\\n",
    "\\boldsymbol{\\Sigma}_c^{(t+1)} &= \\frac{\\sum_{i = 1}^{n} r_{ic}^{(t + 1)} \\left(\\mathbf{x}_i - \\boldsymbol{\\mu}_c^{(t+1)}\\right) \\left(\\mathbf{x}_i - \\boldsymbol{\\mu}_c^{(t+1)}\\right)^\\top}{\\sum_{i = 1}^{n} r_{ic}^{(t)}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "***\n",
    "**Exercise:** Consider a GMM where the covariance matrices for all components are _identical_ and _isotropic_, i.e. $\\boldsymbol{\\Sigma}_c = \\varepsilon \\mathbb{I}$ for all $c \\in \\{1, \\ldots, k\\}$ where $\\mathbb{I}$ is the identity matrix and $\\varepsilon > 0$ is a fixed parameter.\n",
    "Show that the E- and M-steps for the GMM are equivalent to the k-means algorithm (a.k.a. the Lloydâ€“Forgy algorithm) in the limit $\\varepsilon \\to 0$.\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90051 Workshop 7\n",
    "## Variational autoencoders (VAEs)\n",
    "***\n",
    "\n",
    "In this worksheet, we'll implement a _variational autoencoder (VAE)_ as introduced by [Kingma & Welling (2013)](https://arxiv.org/pdf/1312.6114.pdf).\n",
    "We'll use an independent Bernoulli likelihood for the data $\\mathbf{x}$, with a spherical Gaussian prior on the latent variable $\\mathbf{z}$.\n",
    "We'll adopt a convolutional architecture for the encoder/decoder neural nets, which is appropriate for image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 108\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from packaging import version\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__) >= version.parse(\"2.2\"), \\\n",
    "    \"This notebook requires TensorFlow 2.2 or above.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Binarised MNIST data\n",
    "\n",
    "We'll reuse the MNIST data introduced in the last worksheet. \n",
    "However, we need to **binarise** the images, since our generative model makes the simplifying assumption that each pixel is binary (black/white) rather than 8-bit greyscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Binarise images\n",
    "x_train = (x_train >= 128).astype('float32')\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = (x_test >= 128).astype('float32')\n",
    "x_test = np.expand_dims(x_test, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we illustrate a binary image of a \"5\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0], cmap='Greys_r')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Specification of the VAE\n",
    "\n",
    "Following the original VAE paper [(Kingma & Welling, 2013)](https://arxiv.org/pdf/1312.6114.pdf), we assume a latent variable model $p_\\theta(\\mathbf{x}, \\mathbf{z})$ where:\n",
    "\n",
    "* $p_\\theta(\\mathbf{x}|\\mathbf{z}) = \\prod_{i = 1}^{d} \\pi_i^{x_i} (1 - \\pi_i)^{1 - x_i}$ (an independent Bernoulli likelihood for each pixel)\n",
    "* $\\boldsymbol{\\pi} = (\\pi_1, \\ldots, \\pi_d) = \\operatorname{DecoderNN}_\\theta(\\mathbf{z})$ (Bernoulli probabilities are parameterised by a decoder neural network)\n",
    "* $p_\\theta(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z}; 0, \\mathbf{I})$ (a spherical Gaussian prior on $\\mathbf{z}$)\n",
    "\n",
    "The posterior approximation $q_\\phi(\\mathbf{z}|\\mathbf{x})$ is taken to be a factorised Gaussian:\n",
    "\n",
    "* $q_\\phi(\\mathbf{z}|\\mathbf{x}) = \\mathcal{N}(\\mathbf{z}; \\boldsymbol{\\mu}, \\operatorname{diag}(\\boldsymbol{\\sigma}))$\n",
    "* $(\\boldsymbol{\\mu}, \\log \\boldsymbol{\\sigma}) = \\operatorname{EncoderNN}_\\phi(\\mathbf{x})$ (mean and covariance are parameterised by an encoder neural network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoder and decoder neural networks\n",
    "\n",
    "In this section, we'll implement the encoder and decoder neural networks, which are denoted by $\\operatorname{EncoderNN}_\\phi$ and $\\operatorname{DecoderNN}_\\theta$ above.\n",
    "We'll use a convolutional architecture, since we're working with images.\n",
    "\n",
    "For the encoder network, we'll use two convolutional layers, followed by separate fully-connected layers for the mean ($\\boldsymbol{\\mu}$) and variance ($\\log \\boldsymbol{\\sigma}$) parameters.\n",
    "Since the network has more than one output ($\\boldsymbol{\\mu}$ and $\\log \\boldsymbol{\\sigma}$) we can't use the basic [Sequential API](https://keras.io/guides/sequential_model/) in Keras.\n",
    "Instead, we'll use the more general [Functional API](https://keras.io/guides/functional_api/). \n",
    "With the Functional API, we start with `Input` layers, chain layer calls to create outputs and then instantiate a `Model` from the inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 2   # assume z lives in a 2D space (easy to visualise later)\n",
    "\n",
    "encoder_input = layers.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, 4, strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\n",
    "x = layers.Conv2D(64, 4, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "z_mean = layers.Dense(LATENT_DIM, name='z_mean')(x)\n",
    "z_log_var = layers.Dense(LATENT_DIM, name='z_log_var')(x)\n",
    "encoder = keras.Model(inputs=encoder_input, outputs=[z_mean, z_log_var], name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the decoder network, we use a fully-connected layer, followed by three convolutional transpose (deconvolutional) layers.\n",
    "Since there's only one output ($\\boldsymbol{\\pi}$), we can use the Sequential API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = keras.Sequential([\n",
    "    keras.Input(shape=(LATENT_DIM,)),\n",
    "    layers.Dense(7 * 7 * 64, activation=\"relu\"),\n",
    "    layers.Reshape((7, 7, 64)),\n",
    "    layers.Conv2DTranspose(32, 4, activation=\"relu\", strides=2, padding=\"same\"),\n",
    "    layers.Conv2DTranspose(16, 4, activation=\"relu\", strides=2, padding=\"same\"),\n",
    "    layers.Conv2DTranspose(1, 4, activation=\"sigmoid\", strides=1, padding=\"same\")\n",
    "], name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Objective function\n",
    "\n",
    "We want to _maximise_ the evidence lower bound (ELBO), or equivalently _minimise_ the negative ELBO.\n",
    "In the workshop slides, we saw that the negative ELBO can be written as:\n",
    "\n",
    "$$\n",
    "-\\mathcal{L}_{\\theta, \\phi}(\\mathbf{x}) = - \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})}[\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] - D_{\\mathrm{KL}}[q_\\phi(\\mathbf{z}|\\mathbf{x}) \\| p_\\theta(z)]\n",
    "$$\n",
    "\n",
    "_Note: the first term can approximated by a single sample $\\mathbf{z}^\\star$ from $q_\\phi(\\mathbf{z}|\\mathbf{x})$, in which case it simplifies to $\\log p_\\theta(\\mathbf{x}|\\mathbf{z}^\\star)$._\n",
    "\n",
    "***\n",
    "**Exercise:** Show that the two terms in the expression for the negative ELBO can be written as:\n",
    "$$\n",
    "- \\log p_\\theta(\\mathbf{x}|\\mathbf{z}^\\star) = \\sum_{i = 1}^{d} \\left\\{- x_i \\log (\\pi_i^\\star) - (1 - x_i) \\log (1 - \\pi_i^\\star) \\right\\} \\ \\text{with} \\ \\boldsymbol{\\pi}^\\star = \\operatorname{DecoderNN}_\\theta(\\mathbf{z}^\\star)\n",
    "$$\n",
    "\n",
    "where $d$ is the dimension of $\\mathbf{x}$ (in this case $d = 784$) and\n",
    "\n",
    "$$\n",
    "- D_{\\mathrm{KL}}(q_\\phi(\\mathbf{z}|\\mathbf{x}) \\| p_\\theta(\\mathbf{z})) = \\frac{1}{2} \\sum_{i = 1}^{m} \\left\\{ 1 + \\log (\\sigma_i^2) - \\mu_i^2 - \\sigma_i^2 \\right\\}\n",
    "$$\n",
    "\n",
    "where $m$ is the dimension of $\\mathbf{z}$ (in this case $m = 2$).\n",
    "***\n",
    "\n",
    "We implement these terms as separate loss functions below using TensorFlow ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def loss_kl_term(z_mean, z_log_var):\n",
    "    \"\"\"KL divergence contribution to the total loss\n",
    "    \n",
    "    Args:\n",
    "        z_mean: A tensor with shape [n_samples, n_latent_dim]\n",
    "        z_log_var: A tensor with shape [n_samples, n_latent_dim]\n",
    "    \n",
    "    Returns:\n",
    "        A scalar tensor\n",
    "    \"\"\"\n",
    "    loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "    # Add up contributions from each latent dimension\n",
    "    loss = -0.5 * tf.reduce_sum(loss, axis=-1)\n",
    "    # Average over samples\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "@tf.function()\n",
    "def loss_reconstruction_term(x, x_reconstructed):\n",
    "    \"\"\"Reconstruction contribution to the total loss\n",
    "    \n",
    "    Args:\n",
    "        x: A tensor with shape [n_samples, ...]\n",
    "        x_reconstructed: A tensor with shape [n_samples, ...]\n",
    "    \n",
    "    Returns:\n",
    "        A scalar tensor\n",
    "    \"\"\"\n",
    "    loss = keras.backend.binary_crossentropy(x, x_reconstructed)\n",
    "    # Add up contributions from each pixel\n",
    "    loss = tf.reduce_sum(loss, axis=[1,2,3])\n",
    "    # Average over samples\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the VAE\n",
    "\n",
    "We've now prepared the main components we need for the VAE: the encoder and decoder neural nets, as well as custom loss functions for training.\n",
    "In this section, we'll put the components together.\n",
    "Below we define a new `VAE` model class, which will hold the encoder and decoder and coordinate training.\n",
    "Since training is non-standard, we need to override the `train_step` method (this is the code that runs for each step of stochastic gradient descent).\n",
    "\n",
    "There are a couple of important points to note about the code:\n",
    "\n",
    "* We need to direct TensorFlow to record operations that are applied in the forward pass, so that we can do backpropagation. In TensorFlow 2, operations can be recorded by running them in the scope of a [GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape).\n",
    "* Since we can't do backpropagation through a sampling step, we apply the reparameterisation trick. Specifically, we compute $\\mathbf{z} = \\boldsymbol{\\mu} + \\boldsymbol{\\sigma} \\odot \\boldsymbol{\\epsilon}$ where $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\mathbf{I})$ in place of $\\mathbf{z} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\operatorname{diag}(\\boldsymbol{\\sigma}))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        # Record operations to tape \n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var = self.encoder(data)\n",
    "            \n",
    "            # Reparameterisation trick to sample z\n",
    "            epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "            z = z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "            reconstruction = self.decoder(z)\n",
    "            \n",
    "            # Compute contributions to the loss\n",
    "            r_loss = loss_reconstruction_term(data, reconstruction)\n",
    "            kl_loss = loss_kl_term(z_mean, z_log_var)\n",
    "            total_loss = r_loss + kl_loss\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        \n",
    "        # Update trainable weights\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": r_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to instantiate the model and fit it on the training data. \n",
    "This may take 10-15 min when running on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer='adam')\n",
    "vae_history = vae.fit(x_train, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualising the latent space\n",
    "\n",
    "Since one of the main applications of VAEs is representation learning, it is useful to understand the structure of the latent (representation) space.\n",
    "This is not too difficult in our case, since we chose to use a 2D latent space which is easy to visualise.\n",
    "\n",
    "In the code block below, we apply the decoder on a rectangular grid of points in the latent space, plotting the expected image obtained at each point (when passed through the decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def plot_latent(vae, num_images, figsize=(15,15)):\n",
    "    _, image_height, image_width, _ = vae.decoder.output_shape\n",
    "    num_images = 29\n",
    "\n",
    "    # Create equal probability density grid on 2D latent space\n",
    "    z_grid = norm().ppf(np.linspace(0.01, 0.99, num_images))\n",
    "    z_0, z_1 = np.meshgrid(z_grid, z_grid[::-1])\n",
    "\n",
    "    # Apply decoder on grid\n",
    "    z = np.c_[z_0.ravel(), z_1.ravel()]\n",
    "    images = vae.decoder(z)\n",
    "\n",
    "    # Reorganise into num_images x num_images grid\n",
    "    images = tf.reshape(images, (num_images, num_images, image_height, image_width))\n",
    "    images = tf.transpose(images, perm=[0,2,1,3])\n",
    "    images = tf.reshape(images, (num_images * image_height, num_images * image_width))\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(images, cmap=\"Greys_r\")\n",
    "    ticks = np.arange(num_images) * image_width + image_width/2\n",
    "    labels = [\"{:.2f}\".format(l) for l in z_grid]\n",
    "    plt.xticks(ticks=ticks[::2], labels=labels[::2])\n",
    "    plt.yticks(ticks=ticks[::2], labels=labels[::-2])\n",
    "    plt.xlabel(\"$z_0$\")\n",
    "    plt.ylabel(\"$z_1$\")\n",
    "    plt.show()\n",
    "\n",
    "plot_latent(vae, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Question:** Does the VAE do a good job at representing all types of digits? How would you expect the results to change if the latent dimension $m$ was set to 3 instead of 2?\n",
    "***\n",
    "\n",
    "Since we have class labels for the training instances, we can generate another visualisation that shows how the classes are organised in the latent space.\n",
    "Specifically, we apply the encoder to all images in the training set, plot the corresponding latent codes as points, and colour the points according to the class label.\n",
    "This shows similar information to the previous plot, but it makes the overlap between classes more obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, _ = vae.encoder(x_train)\n",
    "z_mean = z_mean.numpy()\n",
    "plt.figure(figsize=(10,10))\n",
    "for digit in range(10):\n",
    "    plt.scatter(z_mean[y_train==digit, 0], z_mean[y_train==digit, 1], label=digit, marker=\".\")\n",
    "plt.legend(title=\"Digit\")\n",
    "plt.xlabel('$z_0$')\n",
    "plt.ylabel('$z_1$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reconstructing images\n",
    "\n",
    "In this section, we interpret the VAE as an autoencoder which is attempting to output an accurate reconstruction of an input image $\\mathbf{x}$.\n",
    "Since the encoder $q_\\phi(\\mathbf{z}|\\mathbf{x})$ and decoder $p_\\theta(\\mathbf{x}|\\mathbf{z})$ both output **random** variables, we need to decide on a method to collapse the random variable to a point.\n",
    "For the encoder, we take the mean $\\bar{\\mathbf{z}} = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})}(\\mathbf{z})$.\n",
    "For the decoder, we consider three options:\n",
    "\n",
    "* draw a sample from $p(\\mathbf{x}|\\bar{\\mathbf{z}})$ (2nd row of plot)\n",
    "* take the mode $\\arg \\max_{\\mathbf{x}} p(\\mathbf{x}|\\bar{\\mathbf{z}})$ (3rd row of plot)\n",
    "* take the expected value $\\mathbb{E}_{p(\\mathbf{x}|\\bar{\\mathbf{z}})}(\\mathbf{x})$ (4th row of plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 10\n",
    "# randomly select some images from the training set\n",
    "train_ids = np.random.choice(x_train.shape[0], size=num_images, replace=False)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=4, ncols=num_images + 1, figsize=(num_images, 0.4*num_images))\n",
    "for i in range(num_images):\n",
    "    # Plot original image\n",
    "    x = x_train[train_ids[i]]\n",
    "    ax[0,i].imshow(x, cmap=\"Greys_r\")\n",
    "    ax[0,i].axis('off')\n",
    "        \n",
    "    z_mean, _ = vae.encoder(np.expand_dims(x, 0))\n",
    "    \n",
    "    # Plot sample from decoder\n",
    "    pi = vae.decoder(z_mean)\n",
    "    x_sample = tf.cast(tf.random.uniform(pi.shape) <= pi, tf.float32)\n",
    "    ax[1,i].imshow(tf.squeeze(x_sample), cmap='Greys_r')\n",
    "    ax[1,i].axis('off')\n",
    "    \n",
    "    # Plot mode of decoder\n",
    "    x_mode = vae.decoder(z_mean) >= 0.5\n",
    "    ax[2,i].imshow(tf.squeeze(x_mode), cmap='Greys_r')\n",
    "    ax[2,i].axis('off')\n",
    "    \n",
    "    # Plot mean of decoder\n",
    "    x_mean = vae.decoder(z_mean)\n",
    "    ax[3,i].imshow(tf.squeeze(x_mean), cmap='Greys_r')\n",
    "    ax[3,i].axis('off')\n",
    "    \n",
    "ax[0,num_images].text(0, 0.4, 'Original')\n",
    "ax[0,num_images].axis('off')\n",
    "ax[1,num_images].text(0, 0.4, 'Reconstructed: sample')\n",
    "ax[1,num_images].axis('off')\n",
    "ax[2,num_images].text(0, 0.4, 'Reconstructed: mode')\n",
    "ax[2,num_images].axis('off')\n",
    "ax[3,num_images].text(0, 0.4, 'Reconstructed: mean')\n",
    "ax[3,num_images].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Drawing from the marginal distribution\n",
    "\n",
    "***\n",
    "**Exercise:** Write code that draws a random image from the marginal $p_\\theta(\\mathbf{x}) = \\int p_\\theta(\\mathbf{x}|\\mathbf{z}) p_\\theta(\\mathbf{z}) \\, \\mathrm{d}\\mathbf{z}$. Comment on the quality of the samples.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

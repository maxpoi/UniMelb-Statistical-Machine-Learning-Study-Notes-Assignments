{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90051 Workshop 8\n",
    "## Recurrent neural networks (RNNs)\n",
    "***\n",
    "\n",
    "In this worksheet, we'll implement a _recurrent neural network (RNN)_ for sentiment analysis of movie reviews. The input to the network will be a movie review, represented as a string, and the output will be a binary label which is \"1\" if the sentiment is _positive_ and \"0\" if the sentiment is _negative_. In practice, a network like this could be applied to social media monitoring—e.g. tracking customer's perceptions of a new product, or prioritising support for disgruntled customers.\n",
    "\n",
    "By the end of this worksheet, you should be able to:\n",
    "\n",
    "* convert raw text to a vector representation for input into a neural network\n",
    "* build a neural network with a recurrent architecture to take advantage of the _order_ of words in text\n",
    "* use `tf.data` to feed data into neural network as an alternative to NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 108\n",
    "\n",
    "import tensorflow as tf\n",
    "from packaging import version\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__) >= version.parse(\"2.3\"), \\\n",
    "    \"This notebook requires TensorFlow 2.3 or above.\"\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Large Movie Review (IMDb) Dataset\n",
    "\n",
    "In order to train and evaluate our sentiment analysis model, we'll use the _Large Movie Review Dataset_. \n",
    "It contains 50,000 movie reviews crawled from the [IMDb website](https://www.imdb.com/), split evenly into train/test sets.\n",
    "The dataset excludes _neutral reviews_ (with a rating $> 4/10$ and $< 7/10$) and defines:\n",
    "\n",
    "* a _positive review_ as having a rating $\\geq 7/10$ \n",
    "* a _negative review_ as having a rating $\\leq 4/10$.\n",
    "\n",
    "More information about the dataset is available [here](https://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "We'll download the dataset using the [TensorFlow Datasets (TFDS) library](https://www.tensorflow.org/datasets/overview), which provides access to a variety of machine learning datasets.\n",
    "First, we need to install the TFDS library using `pip`, since it's not included in core TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing the library, we can use it to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "(train_ds, test_ds), ds_info = tfds.load(name='imdb_reviews', split=('train', 'test'), as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_ds` and `test_ds` variables represent the train/test datasets respectively, however **they aren't stored in NumPy arrays as we're used to**.\n",
    "Instead, the TFDS library returns a special type of object called a [TensorFlow Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n",
    "A `Dataset` offers two main advantages over NumPy arrays:\n",
    "\n",
    "1. It can represent large datasets _without reading all of the data into memory_. This is important for large-scale applications, where the dataset must be streamed from disk into memory.\n",
    "2. It represents the output of a _data pipeline_ (which may include transformations) rather than the data itself.\n",
    "\n",
    "Since a `Dataset` is not guaranteed to completely exist in memory, random access (i.e. indexing) is not supported.\n",
    "Instead, a `Dataset` behaves more like a Python iterable—we can only request the _next_ element, which may be a single instance or a batch of instances.\n",
    "\n",
    "Let's iterate over the first 3 elements in `train_ds` and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_ds.take(3).as_numpy_iterator():\n",
    "    # decode binary string to UTF-8\n",
    "    review = x.decode()\n",
    "    # convert integer binary label to string\n",
    "    label = \"positive\" if y == 1 else \"negative\"\n",
    "    print(\"A {} review:\\n\".format(label), review, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Question:** Based on the above training examples, what features do you think will be important for predicting positive/negative sentiment? If the words in the text were randomly shuffled, do you think accuracy would suffer?\n",
    "***\n",
    "\n",
    "When working with Datasets, batching and random shuffling are treated as steps in the data input pipeline.\n",
    "In the code block below, we add these steps to the pipeline in preparation for pre-processing and training:\n",
    "\n",
    "* First, we cache the entire dataset in memory. This is okay since the dataset is relatively small.\n",
    "* Second, we randomly shuffle the instances.\n",
    "* Third, we group the instances into batches of size 128.\n",
    "* Fourth, we call prefetch to reduce latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache()\n",
    "train_ds = train_ds.shuffle(ds_info.splits['train'].num_examples)\n",
    "train_ds = train_ds.batch(128)\n",
    "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_ds = test_ds.cache()\n",
    "test_ds = test_ds.shuffle(ds_info.splits['test'].num_examples)\n",
    "test_ds = test_ds.batch(128)\n",
    "test_ds = test_ds.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text preprocessing\n",
    "\n",
    "In order to feed textual data into a neural network, we need to transform it into a vector representation.\n",
    "This is often done in two steps:\n",
    "\n",
    "1. **Tokenisation.**\n",
    "The text is split into a sequence of words/tokens. \n",
    "For example, when splitting on white space, the text \"This was an absolutely terrible movie\" would become \\[\"This\", \"was\", \"an\", \"absolutely\", \"terrible\", \"movie\"\\].\n",
    "\n",
    "2. **Word vectorisation/embedding.** \n",
    "Individual words/tokens are mapped to a vector representation. \n",
    "We'll start with a simple one-hot vector encoding, and map to a lower-dimensional representation later on.\n",
    "\n",
    "For the one-hot vector encoding, we choose a vocabulary of size $K$.\n",
    "Each word in the vocabulary is associated with a dimension in the space, or equivalently a word id in the set $\\{0, \\ldots, K-1\\}$.\n",
    "Word ids 0 and 1 are reserved to represent _padding_ and _unknown words_ (also known as out-of-vocabulary words), respectively.\n",
    "\n",
    "In between steps 1 and 2 above, we'll also do some **standardisation**:\n",
    "\n",
    "* we'll convert text to lowercase\n",
    "* we'll remove HTML break tags \"\\<br /\\>\" (which you might have noticed in the third review we printed out above)\n",
    "* we'll remove punctuation characters\n",
    "\n",
    "We can perform all of these steps using a built-in layer from Keras called [`TextVectorization`](https://keras.io/api/layers/preprocessing_layers/core_preprocessing_layers/text_vectorization/).\n",
    "It does everything we need by default, apart from the standardisation, which we need to specify manually.\n",
    "\n",
    "In the code block below, we define a function to perform standardisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def custom_standardisation(input_data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_data : a Tensor of shape (batch_size,)\n",
    "    \n",
    "    Return:\n",
    "        a Tensor of shape (batch_size,)\n",
    "    \"\"\"\n",
    "    # Convert string to lowercase\n",
    "    input_data = tf.strings.lower(input_data)\n",
    "    \n",
    "    # Remove break tags\n",
    "    input_data = tf.strings.regex_replace(input_data, \"<br />\", \" \")\n",
    "    \n",
    "    # Remove punctuation\n",
    "    punctuation_pattern = \"[%s]\" % re.escape(string.punctuation)\n",
    "    input_data = tf.strings.regex_replace(input_data, punctuation_pattern, \"\")\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the `TextVectorization` layer. \n",
    "Note that we're limiting the size of the vocabulary to $K = 5000$. \n",
    "We'll also require that the sequences be of uniform length 300 to improve efficiency.\n",
    "If a review contains more than 300 words, only the first 300 words will be kept.\n",
    "If a review contains less than 300 words, the remaining \"slots\" will be filled with padding (id 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "MAX_LEN = 300\n",
    "vectorise_layer = TextVectorization(max_tokens=VOCAB_SIZE, output_mode=\"int\", \n",
    "                                    standardize=custom_standardisation, \n",
    "                                    output_sequence_length=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the vocabulary for the layer, we need to call the `adapt` method and pass in some training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset without class labels\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorise_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the learned vocabulary using the `get_vocabulary` method. \n",
    "The most frequently occurring words are assigned the smallest integer ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorise_layer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our `TextVectorization` layer, let's apply it to the first batch of training data and print out the first 3 examples in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in text_ds.take(1).map(vectorise_layer).as_numpy_iterator():\n",
    "    print(batch[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RNN-based classifier\n",
    "\n",
    "Now that we've prepared a layer for preprocessing, we're ready to implement the neural network.\n",
    "The network will consist of three main components:\n",
    "\n",
    "1. **Embedding layer.** This layer will map word vectors from a sparse one-hot encoded representation to a dense lower-dimensional embedding space. It is effectively equivalent to a dense layer with a linear activation function.\n",
    "2. **RNN layer.** This layer will sequentially apply an RNN cell to the densely-encoded word vectors. Since we're doing classification, we only need to use the output at the _end of the sequence_. In other applications, such as language modelling, we would use the output from _each element of the sequence_.\n",
    "3. **Dense layers.** These layers will process the final output of the RNN layer to produce a sentiment classification.\n",
    "\n",
    "***\n",
    "**Exercise:** Complete the following code block to implement an RNN-based sentiment classifier. You should add three layers: \n",
    "\n",
    "* an `LSTM` layer with `L_UNITS = 32` units followed by \n",
    "* a `Dense` layer with `D1_UNITS = 32` units, and \n",
    "* a `Dense` layer with 1 unit. \n",
    "\n",
    "\\[Note: you may like to experiment with your own architecture. For example, you could use an `SimpleRNN` layer in place of a `LSTM` layer, or a `Bidirectional` RNN.\\]\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_EMBED = 32\n",
    "L_UNITS = 32\n",
    "D1_UNITS = 32\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # Specify that the model will accept a 1D tensor of strings as input with \n",
    "    # an unknown batch size\n",
    "    keras.Input(shape=(None,), dtype=tf.string),\n",
    "    \n",
    "    # Vectorise the strings using the preprocessing layer we defined above\n",
    "    vectorise_layer,\n",
    "    \n",
    "    # Map the sequence of one-hot encoded integer vectors to a \n",
    "    # lower-dimensional embedding space. By setting `mask_zero = True`, we \n",
    "    # instruct Keras to apply a mask so that downstream layers can ignore \n",
    "    # padded parts of the sequence.\n",
    "    layers.Embedding(VOCAB_SIZE, DIM_EMBED, mask_zero=True),\n",
    "    \n",
    "    # Sequentially apply an LSTM cell to the sequence of word embeddings, \n",
    "    # returning only the final output\n",
    "    ..., # fill in\n",
    "    \n",
    "    # Apply two dense layers to output the probability of a positive sentiment\n",
    "    ..., # fill in\n",
    "    ... # fill in\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the final output is a scalar corresponding to the probability of a positive sentiment, we'll minimise the binary cross-entropy loss.\n",
    "\n",
    "In the code block below, we compile the model and run training for 15 epochs.\n",
    "Training may take 10-15 min to complete on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              optimizer=keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_ds, validation_data=test_ds, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot loss curves to check for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history['loss'], label='Train')\n",
    "plt.plot(history.epoch, history.history['val_loss'], label='Test')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss curves show that the model is overfitting, as the training loss has dropped below the test loss.\n",
    "\n",
    "***\n",
    "**Question:** What actions can we take to prevent overfitting? What could be the main cause of overfitting in this case?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the model\n",
    "\n",
    "Since we integrated text prepocessing into the model, it's straightforward to apply it to new test instances. \n",
    "We can simply pass in a NumPy array of strings, and immediately get sentiment predictions.\n",
    "\n",
    "***\n",
    "**Exercise:** Test the sentiment classification model on a movie review of your own.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: visualising the embedding space (optional)\n",
    "\n",
    "***\n",
    "**Exercise:** Run the code block below to save the word dictionary and weights in the `Embedding` layer as TSV files. You can then load these files in the [Embedding Projector](http://projector.tensorflow.org/) web app to visualise the embedding space.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "tokens = model.layers[0].get_weights()[0]\n",
    "ids = model.layers[0].get_weights()[1]\n",
    "tokens = tokens[np.argsort(ids)]\n",
    "ids = ids[np.argsort(ids)]\n",
    "embedding_weights = model.layers[1].get_weights()[0]\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for idx, token in zip(ids[2::], tokens[2::]):\n",
    "    vec = embedding_weights[idx]\n",
    "    out_m.write(token.decode() + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
